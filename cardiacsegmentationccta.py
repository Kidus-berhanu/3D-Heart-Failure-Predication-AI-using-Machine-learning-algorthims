# -*- coding: utf-8 -*-
"""cardiacSegmentationCCTA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ZNxgmzR-sgAvCQwxGDbpzaL-giNO33G

# Automatic cardiac segmentation in CCTA
This repository contains code for training and inference of a 3D CNN for whole-heart segmentation in CCTA. Please cite the following paper if you use this code.

Steffen Bruns, Jelmer M Wolterink, Thomas PW van den Boogert, José P Henriques, Jan Baan, R Nils Planken, and Ivana Išgum. Automatic whole-heart segmentation in 4D TAVI treatment planning CT. In Medical Imaging 2021: Image Processing, volume 11596, page 115960B. International Society for Optics and Photonics, 2021. (https://doi.org/10.1117/12.2581020)

This project was financially supported by Philips Healthcare.

## Usage
All functionality is implemented in the python file cardiacSegmentationCCTA.py. It can perform either CNN training or inference.

### A) CNN training
For CNN training, CT images in .mhd format are required. Additionally, reference segmentations with the same size and format are required. In the reference segmentation, each voxel of class i should be assigned the value i (with 0 for background), e.g. 0: background, 1: left ventricular cavity, 2: right ventricle, 3: left atrium, 4: right atrium, 5: left ventricular myocardium (NCLASS=6). If CT images and reference segmentations are prepared, follow these steps to train a CNN:
1) Make sure the training and validation images and reference segmentations are located in the correct relative paths, e.g.
- code in: /content/Segmentation
- training images in: /content/Segmentation/mmwhs/fold_01/train/images
- training reference in: /content/Segmentation/mmwhs/fold_01/train/reference
- validation images in: /content/Segmentation/mmwhs/fold_01/validate/images
- validation reference in: /content/Segmentation/mmwhs/fold_01/validate/reference
2) Start the program from the command line and supply the arguments for the argument parser, e.g.

python cardiacSegmentationCCTA.py --mode train --train_dir mmwhs --lr 0.001 --lr_step_size 4000 --lr_gamma 0.3 --n_iterations 10000 --n_class 6 --vox_size 0.8 --batch_size 8 --fold 1 --rand_seed 123 --tag testCardiacSegmentationCCTA

(Some of the arguments are optional. Additional information on the specific arguments can be found in the code itself.)

### B) CNN inference
For CNN inference, CT images in .mhd format and a trained network are required. Follow these steps to perform CNN inference:
1) Make sure the CT images for inference and the trained network(s) are located in the correct relative paths, e.g.
- code in: /content/Segmentation/src
- CT images in: /content/Segmentation/mmwhs/fold_01/test/images
- trained network(s) in: /content/Segmentation/experiments/testCardiacSegmentationCCTA
2) Start the program from the command line and supply the arguments for the argument parser, e.g.

python cardiacSegmentationCCTA.py --mode test --trained_networks /home/user/Segmentation/experiments/testCardiacSegmentationCCTA/10000.pt --test_dir /home/user/Segmentation/mmwhs/fold_01/test/images --n_class 6 --vox_size 0.8

(Some of the arguments are optional. Additional information on the specific arguments can be found in the code itself.)

# Program Configuration
"""

#@title Mode Config
mode = "test" #@param ["train", "test"]

#@title Training Specific Configuration

# Only used for mode='train': tag determines the name of the directory in which the networks will be saved.
tag = "testCardiacSegmentationCCTA" #@param {type:"string"}

# Only used for mode='train': directory in which the training images are saved.
train_dir = "mmwhs" #@param {type:"string"}

# Only used for mode='train': the training image fold which will be used for training.
fold = 1 #@param {type:"integer"}

# Sets the (initial) learning rate
lr = 0.001 #@param {type:"number"}

# Sets the number of iterations after which the learning rate is reduced.
lr_step_size = 4000 #@param {type:"integer"}

# Sets the factor with which the learning rate is reduced every X steps. Must be between 0 and 1
lr_gamma = 0.3 #@param {type:"number"}

# Sets the total number of iterations for training
n_iterations = 1000 #@param {type:"slider", min:100, max:30000, step:100}
n_iterations = n_iterations + 1
# Sets the mini-batch size for training.
batch_size = 4 #@param {type:"integer"}

#@title Testing Specific Configuration

# Specify the trained networks you want to use. If you specify more than one network, these will be used as an ensemble to produce only one output segmentation per input test image.
trained_networks = "/content/Segmentation/experiments/testCardiacSegmentationCCTA/" #@param {type:"string"}
trained_networks =  trained_networks + str(n_iterations - 1) + ".pt"


# Only used for mode='test': directory of the images to be tested
test_dir = "/content/Segmentation/mmwhs/fold_1/test/images" #@param {type:"string"}

#@title Global Configuration

# Download the trained networks that are saved to the google drive
download_trained_networks = True #@param {type:"boolean"}

# Sets the random seed for numpy and torch
rand_seed = 115 #@param {type:"integer"}

# Sets the (isotropic) voxel size the network operates on
vox_size = 0.8 #@param {type:"number"}

# Sets the number of classes to be segmented including background
n_class = 6 #@param {type:"integer"}

# Only for testing: Set to True if labels are present and Dice scores should be calculated
labels_present = True #@param {type:"boolean"}

# Sets percentage of dataset to use for Testing
testing_percent = 20 #@param {type:"slider", min:0, max:35, step:5}

# Sets percentage of dataset to use for Validation
validation_percent = 10 #@param {type:"slider", min:0, max:30, step:5}

!pip install SimpleITK
!pip install visdom
!pip install torch
!pip install nibabel
!pip install matplotlib
!pip install google-colab

"""# Program Setup"""

# Commented out IPython magic to ensure Python compatibility.
#@title Imports
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.optim as optim
import sys
import os
import nibabel as nib
import glob
import numpy as np
import SimpleITK as sitk
import visdom
import re
import time
import scipy.ndimage.interpolation as scndy
import argparse
import shutil
import matplotlib.pyplot as plt
from google.colab import drive

# %matplotlib inline

os.getcwd()

paths = ['/content','Segmentation','mmwhs','fold_1','train','validate','test']
root_path = paths[0] + '/' + paths[1] + '/' + paths[2] + '/' + paths[3]
try:
  os.mkdir(paths[0] + '/' + paths[1])
  os.mkdir(paths[0] + '/' + paths[1] + '/' + paths[2])
  os.mkdir(root_path)
  os.mkdir(os.path.join(root_path,paths[4]))
  os.mkdir(os.path.join(root_path,paths[5]))
  os.mkdir(os.path.join(root_path,paths[6]))
except:
   pass


#lis = ['train/images', 'train/reference', 'validate/images', 'validate/reference','test/images']
lis = ['validate/images', 'validate/reference','test/images','test/reference']
  
for items in lis:
  try:
    path = os.path.join(root_path, items)
    os.mkdir(path)
  except:
    pass

drive.mount('/content/gdrive')

# Copy training images and labels into drive

no_files_exist = False

try:
  shutil.copytree('/content/gdrive/MyDrive/490 Heart/Data/CT Masks/labelsTr','/content/Segmentation/mmwhs/fold_1/train/reference')
  shutil.copytree('/content/gdrive/MyDrive/490 Heart/Data/CT Masks/imagesTr','/content/Segmentation/mmwhs/fold_1/train/images')

  if download_trained_networks:
    shutil.copytree(os.path.abspath('./gdrive/MyDrive/490 Heart/modelTr'),'/content/Segmentation/experiments/testCardiacSegmentationCCTA',dirs_exist_ok=True)
  no_files_exist = True
except Exception as e:
  print(e)

# Copy testing images and lables into drive
#! TODO

#@title Setup Variables
"""parser.add_argument('--labels_present', dest='labels_present', default=False, action='store_true',
                    help="Only for testing: Set to True if labels are present and Dice scores should be calculated.")"""

np.random.seed(rand_seed)
torch.manual_seed(rand_seed)

NCLASS = n_class
VOXSIZE = vox_size
LABELPRESENT = labels_present
BATCHSIZE = batch_size

#@title Training Testing Validation Split
if no_files_exist:
  training_dir = os.getcwd() + os.path.sep + 'Segmentation' + os.path.sep + train_dir + os.path.sep + 'fold_' + str(
          fold) + os.path.sep + 'train'
  
  testing_dir = os.getcwd() + os.path.sep + 'Segmentation' + os.path.sep + train_dir + os.path.sep + 'fold_' + str(
          fold) + os.path.sep + 'test'
  
  validation_dir = os.getcwd() + os.path.sep + 'Segmentation' + os.path.sep + train_dir + os.path.sep + 'fold_' + str(
        fold) + os.path.sep + 'validate'
  
  img_path = os.path.sep + 'images' +os.path.sep
  ref_path = os.path.sep + 'reference' +os.path.sep
  file_type = r'*.nii'

  train_glob = glob.glob(training_dir + img_path + os.path.sep + file_type)
  train_glob.sort(key=lambda f: int(re.sub('\D', '', f)))

  if testing_percent > 0:
    test_glob = np.random.choice(train_glob,int(np.floor(len(train_glob) * 0.01 * testing_percent)),replace=False)
    for current_path in test_glob:
      shutil.move(current_path, current_path.replace(training_dir,testing_dir))
      shutil.move(current_path.replace('images','reference').replace('image_','label_'), current_path.replace('images','reference').replace('image_','label_').replace(training_dir,testing_dir))
      train_glob.remove(current_path)
  
  if validation_percent > 0:
    val_glob = np.random.choice(train_glob,int(np.floor(len(train_glob) * 0.01 * validation_percent)),replace=False)
    for current_path in val_glob:
      shutil.move(current_path, current_path.replace(training_dir,validation_dir))
      shutil.move(current_path.replace('images','reference').replace('image_','label_'), current_path.replace('images','reference').replace('image_','label_').replace(training_dir,validation_dir))
      train_glob.remove(current_path)

"""# Functions

"""

#@title MHD to Raw Data Converstion

def load_mhd_to_npy(filename):
    image = sitk.ReadImage(filename)
    spacing = image.GetSpacing()
    return np.swapaxes(sitk.GetArrayFromImage(image), 0, 2), spacing

#@title Image Directory Loading

def loadImageDir3D(imagefiles):
    imagefiles.sort()
    # Images is a list of 3D images
    images = []
    # Labels is a list of 3D masks
    labels = []
    processed = 0

    # Iterate over training images
    for fi in range(0, len(imagefiles)):
        print('Loading ' + str(processed) + '/' + str(len(imagefiles)))
        processed = processed + 1
        f_a = imagefiles[fi]
        print(f_a)
        reffile_a = f_a.replace('images', 'reference').replace('_image', '_label')
        reffile_a = reffile_a.replace('image_','label_')
        print(reffile_a)
        # If reference file exists
        if os.path.isfile(reffile_a):
            # Load image file
            image, spacing = load_mhd_to_npy(f_a)
            image = image.astype('float32')

            # Resample image to isotropic resolution
            image = scndy.zoom(image, (spacing[0] / VOXSIZE, spacing[1] / VOXSIZE, spacing[2] / VOXSIZE),
                               order=0)

            # Scale image intensities to [0, 1] range
            image[image < -1024.0] = -1024.0
            image[image > 3071.0] = 3071.0
            image = (image + 0.0) / 4096.0

            # Load reference file
            ref, spacing_a = load_mhd_to_npy(reffile_a)

            # Resample reference to isotropic resolution
            ref = scndy.zoom(ref, (spacing[0] / VOXSIZE, spacing[1] / VOXSIZE, spacing[2] / VOXSIZE),
                             order=0)

            images.append(image)
            labels.append(ref)
        else:
            print("Error: Reference file " + reffile_a + " does not exist.")
            sys.exit()

    return images, labels

#@title Loss function
class DiceLossSimple3D(nn.Module):
    def __init__(self):
        super(DiceLossSimple3D, self).__init__()
        self.dice_loss = dice_loss_simple

    def forward(self, inputs, targets):
        sumdice = 0.0
        for c in range(NCLASS):
            sumdice += self.dice_loss(inputs[:, c, :, :, :].contiguous(), targets[:, c, :, :, :].contiguous())
        return sumdice


# Support function for actual loss function
def dice_loss_simple(input, target):
    smooth = 1.0
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))

#@title 3D Batching
def generateBatch3D(images, labels, nsamp=32):
    # Size of each patch 128x128x128 voxels
    sx = 128
    sy = 128
    sz = 128

    # Mini-batch and labels
    batch_im = np.zeros((nsamp, 1, sx, sy, sz))
    batch_la = np.zeros((nsamp, NCLASS, sx, sy, sz))

    for ns in range(nsamp):
        # Randomly select training image
        ind = np.random.randint(0, len(images), 1)
        ind = ind[0]
        imageim = images[ind]
        labelim = labels[ind]

        # Pad image if any direction is smaller than the input patch size
        if imageim.shape[2] < sz:
            offset = sz - imageim.shape[2]
            if offset % 2 == 1:
                offset = offset + 1
            padWidth = (int)(offset / 2)
            imageim = np.pad(imageim, ((0, 0), (0, 0), (padWidth, padWidth)), 'edge')
            labelim = np.pad(labelim, ((0, 0), (0, 0), (padWidth, padWidth)), 'edge')

        if imageim.shape[1] < sy:
            offsety = sy - imageim.shape[1]
            if offsety % 2 == 1:
                offsety = offsety + 1
            padWidthy = (int)(offsety / 2)
            imageim = np.pad(imageim, ((0, 0), (padWidthy, padWidthy), (0, 0)), 'edge')
            labelim = np.pad(labelim, ((0, 0), (padWidthy, padWidthy), (0, 0)), 'edge')

        if imageim.shape[0] < sx:
            offsetx = sx - imageim.shape[0]
            if offsetx % 2 == 1:
                offsetx = offsetx + 1
            padWidthx = (int)(offsetx / 2)
            imageim = np.pad(imageim, ((padWidthx, padWidthx), (0, 0), (0, 0)), 'edge')
            labelim = np.pad(labelim, ((padWidthx, padWidthx), (0, 0), (0, 0)), 'edge')

        offx = np.random.randint(0, imageim.shape[0] - sx + 1)
        offy = np.random.randint(0, imageim.shape[1] - sy + 1)
        offz = np.random.randint(0, imageim.shape[2] - sz + 1)

        imageim = imageim[offx:offx + sx, offy:offy + sy, offz:offz + sz]

        batch_im[ns, 0, :, :, :] = imageim

        labelim = labelim[offx:offx + sx, offy:offy + sy, offz:offz + sz]
        for c in range(NCLASS):
            batch_la[ns, c, :, :, :] = (labelim == c)

    return batch_im, batch_la

#@title Resnet Training

class ResnetBlock3D(nn.Module):
    def __init__(self, dim, padding_type, norm_layer, use_dropout):
        super(ResnetBlock3D, self).__init__()
        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout)

    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout):
        conv_block = []
        assert (padding_type == 'zero')
        p = 1

        conv_block += [nn.Conv3d(dim, dim, kernel_size=3, padding=p),
                       norm_layer(dim, affine=True),
                       nn.ReLU(True)]
        if use_dropout:
            conv_block += [nn.Dropout(0.5)]
        conv_block += [nn.Conv3d(dim, dim, kernel_size=3, padding=p),
                       norm_layer(dim, affine=True)]

        return nn.Sequential(*conv_block)

    def forward(self, x):
        out = x + self.conv_block(x)
        return out


class ResnetGenerator3D(nn.Module):
    def __init__(self, norm_layer=nn.BatchNorm3d, n_blocks=9, ngf=32):
        assert (n_blocks >= 0)
        super(ResnetGenerator3D, self).__init__()
        self.ngf = ngf

        model = [nn.Conv3d(1, self.ngf, kernel_size=7, padding=3),
                 norm_layer(self.ngf, affine=True),
                 nn.ReLU(True)]

        n_downsampling = 2
        for i in range(n_downsampling):
            mult = 2 ** i
            model += [nn.Conv3d(self.ngf * mult, self.ngf * mult * 2, kernel_size=3,
                                stride=2, padding=1),
                      norm_layer(self.ngf * mult * 2, affine=True),
                      nn.ReLU(True)]

        mult = 2 ** n_downsampling
        for i in range(n_blocks):
            model += [ResnetBlock3D(self.ngf * mult, 'zero', norm_layer=norm_layer, use_dropout=False)]

        for i in range(n_downsampling):
            mult = 2 ** (n_downsampling - i)
            model += [nn.ConvTranspose3d(self.ngf * mult, int(self.ngf * mult / 2),
                                         kernel_size=3, stride=2,
                                         padding=1, output_padding=1),
                      norm_layer(int(self.ngf * mult / 2), affine=True),
                      nn.ReLU(True)]

        model += [nn.Conv3d(self.ngf, NCLASS, kernel_size=7, padding=3)]
        model += [nn.Softmax()]

        self.model = nn.Sequential(*model)

    def forward(self, input):
        output = self.model(input)
        return output

"""# Program Bootstraps"""

#@title Training Bootstrap

def train3D(tag, task, fold, lr, lr_step_size, lr_gamma, n_iterations, expdir):
    print('Experiment tag ' + tag)
    net = ResnetGenerator3D(n_blocks=6, ngf=8)
    net.cuda()
    net.float()
    net = nn.DataParallel(net)

    criterion = DiceLossSimple3D()
    optimizer = optim.Adam(net.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=lr_step_size, gamma=lr_gamma)

    # Make sure the training images and reference masks are stored at the correct relative path with respect to the python file
    traindir = os.getcwd() + os.path.sep + 'Segmentation' + os.path.sep + 'mmwhs' + os.path.sep + 'fold_' + str(
        fold) + os.path.sep + 'train'
    imgpath = os.path.sep + 'images' +os.path.sep
    refpath = os.path.sep + 'reference' +os.path.sep
    print("traindir " + traindir)
    filetype = r'*.nii'
    trainimages, trainlabels = loadImageDir3D(glob.glob(traindir + imgpath + os.path.sep + filetype))
    print(trainimages[0].shape,trainlabels[0].shape)
    startIt = -1
    num_epochs = n_iterations
    errors = np.empty(startIt + int(num_epochs) + 1)
    errors[:startIt] = 0
    testes1 = np.empty(startIt + int(num_epochs) + 1)
    testes1[:startIt] = 0

    # visdom can be used for online visualization
    #viz = visdom.Visdom(env=tag, port=8099)
    itInt = 50

    # Make sure the validation images and reference masks are stored at the correct relative path with respect to the python file
    valdir = os.getcwd() + os.path.sep + 'Segmentation' + os.path.sep + task + os.path.sep + 'fold_' + str(
        fold) + os.path.sep + 'validate' + os.path.sep + 'images' +os.path.sep
    valimages, vallabels = loadImageDir3D(glob.glob(valdir + os.path.sep + filetype))

    # A central 128x128x128 voxel patch of one validation image is used for visualization and loss tracking in this case
    valim = valimages[0]
    valims = valim[int(valim.shape[0] / 2) - 64:int(valim.shape[0] / 2) + 64,
             int(valim.shape[1] / 2) - 64:int(valim.shape[1] / 2) + 64,
             int(valim.shape[2] / 2) - 64:int(valim.shape[2] / 2) + 64]
    valimp = valims
    padwidthVal = 0

    if valimp.shape[2] < 128:
        padwidthVal = 128 - valimp.shape[2]
        valimp = np.pad(valimp, ((0, 0), (0, 0), (0, padwidthVal)), 'constant', constant_values=0)
    valimb = np.zeros((1, 1, valimp.shape[0], valimp.shape[1], valimp.shape[2]), dtype='float32')
    valimb[0, 0, :, :, :] = valimp

    valla = vallabels[0]
    valla = valla[int(valim.shape[0] / 2) - 64:int(valim.shape[0] / 2) + 64,
            int(valim.shape[1] / 2) - 64:int(valim.shape[1] / 2) + 64,
            int(valim.shape[2] / 2) - 64:int(valim.shape[2] / 2) + 64]
    if valla.shape[2] < 128:
        valla = np.pad(valla, ((0, 0), (0, 0), (0, padwidthVal)), 'constant', constant_values=0)
    vallab = np.zeros((1, NCLASS, valla.shape[0], valla.shape[1], valla.shape[2]), dtype='float32')
    for cl in range(NCLASS):
        vallab[0, cl, :, :, :] = valla == (cl)

    bs = BATCHSIZE
    # Training loop
    for it in range(num_epochs):
        start = time.time()
        if it > -1:
            optimizer.zero_grad()
            images, labels = generateBatch3D(trainimages, trainlabels, nsamp=bs)
            images, labels = Variable(torch.from_numpy(images).float().cuda()), Variable(
                torch.from_numpy(labels).float().cuda())
            outputstrain = net(images)
            loss = criterion(outputstrain, labels)
            print('Dice loss {}'.format(loss.item()))
            loss.backward()
            scheduler.step()
            optimizer.step()
            errors[it] = loss.item()
        # Validation loss tracking and visualization every itInt iterations
        if it % itInt == 0 and it > 0:
            print(it)
            images_pt = Variable(torch.from_numpy(valimb).float().cuda())
            net.eval()
            outputs_pt = net(images_pt)
            labels_pt = Variable(torch.from_numpy(vallab).float().cuda())
            valloss = criterion(outputs_pt, labels_pt)
            testes1[it:it + itInt] = valloss.item()

            net.train()
            outputs = outputs_pt.float().cpu().data.numpy()

            # Visualization with visdom, not mandatory
            """#viz.image(np.clip(scndy.zoom(
                (np.rot90(np.flipud(np.squeeze(valims[:, :, 63])), k=3, axes=(0, 1)) + 0.25) * 255,
                1.0), 0.0, 255.0),
                win=1,
                opts=dict(title="Validation image"))"""

            """#viz.image(np.clip(
                scndy.zoom((np.rot90(np.flipud(np.squeeze(valla[:, :, 63])), k=3, axes=(0, 1)) + 0.25) * 35, 1.0),
                0.0, 255.0),
                win=2,
                opts=dict(title="Validation labels"))"""

            for cl in range(NCLASS):
                pass
                #viz.image(scndy.zoom(np.clip(np.rot90(np.flipud(np.squeeze(outputs[0, cl, :, :, 63])), k=3, axes=(0, 1)) * (255), 0, 255), 1.0), win=3 + cl, opts=dict(title="Test class {}".format(cl)))

            #vizX = np.zeros((it, 2))
            #vizX[:, 0] = range(it)
            #vizX[:, 1] = range(it)
            #vizY = np.zeros((it, 2))
            #vizY[:, 0] = errors[:it]
            #vizY[:, 1] = testes1[:it]
            #vizY[np.isnan(vizY)] = 0.0
            #viz.line(Y=vizY, X=vizX, win='viswin1')

            netnameout = expdir + os.path.sep + str(it) + '.pt'
            torch.save(net.state_dict(), netnameout)
            np.savetxt(expdir + os.path.sep + 'trainloss.txt', errors[:it])
            np.savetxt(expdir + os.path.sep + 'valloss.txt', testes1[:it])
        print('Iteration {} took {} s'.format(it, time.time() - start))
    print("Training Finished")

#@title Testing BootStrap

def test3D(netnames, imdir):
    filenames = glob.glob(imdir + os.path.sep + r"*.nii")
    print(imdir)
    print(filenames)
    filenames.sort()
    netdir, netbase = os.path.split(netnames[0])
    imageNo = 0
    if len(filenames) > 0:
        for filename in filenames:
            print("Testing " + filename)
            imageNo += 1
            image = sitk.ReadImage(filename)
            spacing_a = image.GetSpacing()
            origin_a = image.GetOrigin()
            direction_a = image.GetDirection()
            image = sitk.GetArrayFromImage(image)
            image = np.swapaxes(image, 0, 2)
            image = image.astype('float32')

            # Store original shape of the image before downsampling
            orshape = image.shape

            image = scndy.zoom(image, (spacing_a[0] / VOXSIZE, spacing_a[1] / VOXSIZE, spacing_a[2] / VOXSIZE),
                               order=0)

            # Intensity scaling to [0, 1] range
            image[image < -1024.0] = -1024.0
            image[image > 3071.0] = 3071.0
            image = (image + 0.0) / 4096.0

            newshape = image.shape

            wx = int(np.floor(image.shape[0] / 4) * 4)
            wy = int(np.floor(image.shape[1] / 4) * 4)
            wz = int(np.floor(image.shape[2] / 4) * 4)
            if wx > 256:
                wx = 256
            if wy > 256:
                wy = 256
            if wz > 256:
                wz = 256

            batch = np.ones((1, 1, wx, wy, wz), dtype='float32')
            outim_os = np.zeros((NCLASS, image.shape[0], image.shape[1], newshape[2]), dtype='float32')

            # Iterate over networks if an ensemble of several networks is used
            for inet in range(len(netnames)):
                print("    Testing with " + netnames[inet])
                net = ResnetGenerator3D(n_blocks=6, ngf=8)
                net = nn.DataParallel(net)
                net.load_state_dict(torch.load(netnames[inet]))
                net.float()
                net.cuda()
                net.eval()

                if wx == 256 or wy == 256 or wz == 256:
                    nx = int((image.shape[0] + 128) / wx) + 1
                    ny = int((image.shape[1] + 128) / wy) + 1
                    nz = int((image.shape[2] + 128) / wz) + 1
                    for ix in range(nx):
                        for iy in range(ny):
                            for iz in range(nz):
                                batch[0, 0, :, :, :] = image[int(ix * int((image.shape[0] - wx) / (nx - 1))):int(
                                    ix * int((image.shape[0] - wx) / (nx - 1))) + wx,
                                                       int(iy * int((image.shape[1] - wy) / (ny - 1))):int(
                                                           iy * int((image.shape[1] - wy) / (ny - 1))) + wy,
                                                       int(iz * int((image.shape[2] - wz) / (nz - 1))):int(
                                                           iz * int((image.shape[2] - wz) / (nz - 1))) + wz]
                                images_pt = Variable(torch.from_numpy(batch).float().cuda())
                                outputs_pt = net(images_pt).float().cpu().data
                                out = outputs_pt.numpy()
                                outim_os[:, int(ix * int((image.shape[0] - wx) / (nx - 1))):int(
                                    ix * int((image.shape[0] - wx) / (nx - 1))) + wx,
                                int(iy * int((image.shape[1] - wy) / (ny - 1))):int(
                                    iy * int((image.shape[1] - wy) / (ny - 1))) + wy,
                                int(iz * int((image.shape[2] - wz) / (nz - 1))):int(
                                    iz * int((image.shape[2] - wz) / (nz - 1))) + wz] = outim_os[:, int(
                                    ix * int((image.shape[0] - wx) / (nx - 1))):int(
                                    ix * int((image.shape[0] - wx) / (nx - 1))) + wx, int(
                                    iy * int((image.shape[1] - wy) / (ny - 1))):int(
                                    iy * int((image.shape[1] - wy) / (ny - 1))) + wy, int(
                                    iz * int((image.shape[2] - wz) / (nz - 1))):int(
                                    iz * int((image.shape[2] - wz) / (nz - 1))) + wz] + np.squeeze(out[0, :, :, :, :])

                else:
                    # Process overlapping patches and add up probabilities
                    batch[0, 0, :, :, :] = image[:wx, :wy, :wz]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, :wx, :wy, :wz] = outim_os[:, :wx, :wy, :wz] + np.squeeze(out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[image.shape[0] - wx:image.shape[0], :wy, :wz]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, image.shape[0] - wx:image.shape[0], :wy, :wz] = outim_os[:,
                                                                                image.shape[0] - wx:image.shape[0],
                                                                                :wy, :wz] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[:wx, image.shape[1] - wy:image.shape[1], :wz]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, :wx, image.shape[1] - wy:image.shape[1], :wz] = outim_os[:, :wx,
                                                                                image.shape[1] - wy:image.shape[1],
                                                                                :wz] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[:wx, :wy, image.shape[2] - wz:image.shape[2]]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, :wx, :wy, image.shape[2] - wz:image.shape[2]] = outim_os[:, :wx, :wy,
                                                                                image.shape[2] - wz:image.shape[
                                                                                    2]] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[image.shape[0] - wx:image.shape[0],
                                           image.shape[1] - wy:image.shape[1],
                                           :wz]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, image.shape[0] - wx:image.shape[0], image.shape[1] - wy:image.shape[1],
                    :wz] = outim_os[
                           :,
                           image.shape[
                               0] - wx:
                           image.shape[
                               0],
                           image.shape[
                               1] - wy:
                           image.shape[
                               1],
                           :wz] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[image.shape[0] - wx:image.shape[0], :wy,
                                           image.shape[2] - wz:image.shape[2]]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, image.shape[0] - wx:image.shape[0], :wy,
                    image.shape[2] - wz:image.shape[2]] = outim_os[
                                                          :,
                                                          image.shape[
                                                              0] - wx:
                                                          image.shape[
                                                              0],
                                                          :wy,
                                                          image.shape[
                                                              2] - wz:
                                                          image.shape[
                                                              2]] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[:wx, image.shape[1] - wy:image.shape[1],
                                           image.shape[2] - wz:image.shape[2]]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, :wx, image.shape[1] - wy:image.shape[1],
                    image.shape[2] - wz:image.shape[2]] = outim_os[
                                                          :, :wx,
                                                          image.shape[
                                                              1] - wy:
                                                          image.shape[
                                                              1],
                                                          image.shape[
                                                              2] - wz:
                                                          image.shape[
                                                              2]] + np.squeeze(
                        out[0, :, :, :, :])

                    batch[0, 0, :, :, :] = image[image.shape[0] - wx:image.shape[0],
                                           image.shape[1] - wy:image.shape[1],
                                           image.shape[2] - wz:image.shape[2]]
                    images_pt = Variable(torch.from_numpy(batch).float().cuda())
                    outputs_pt = net(images_pt).float().cpu().data
                    out = outputs_pt.numpy()
                    outim_os[:, image.shape[0] - wx:image.shape[0], image.shape[1] - wy:image.shape[1],
                    image.shape[2] - wz:image.shape[2]] = outim_os[:, image.shape[0] - wx:image.shape[0],
                                                          image.shape[1] - wy:image.shape[1],
                                                          image.shape[2] - wz:image.shape[2]] + np.squeeze(
                        out[0, :, :, :, :])

            outim_os_or = np.zeros((NCLASS, orshape[0], orshape[1], orshape[2]))
            for k in range(NCLASS):
                # Reshape to original shape
                outim_os_or[k, :, :, :] = scndy.zoom(outim_os[k, :, :, :], (
                    orshape[0] / newshape[0], orshape[1] / newshape[1], orshape[2] / newshape[2]),
                                                     order=0)

            outnamet = netdir + os.path.sep + os.path.split(filename)[-1]

            outim = np.zeros((outim_os_or.shape[1], outim_os_or.shape[2], outim_os_or.shape[3]))
            for z in range(outim_os_or.shape[3]):
                # Calculate output segmentation mask from class probabilities
                outim[:, :, z] = np.argmax(np.squeeze(outim_os_or[:, :, :, z]), axis=0)

            outimt = np.copy(outim)
            outimt = np.swapaxes(outimt, 0, 2)
            outimt = sitk.GetImageFromArray(outimt)
            outimt.SetSpacing(spacing_a)
            outimt.SetOrigin(origin_a)
            outimt.SetDirection(direction_a)
            # Write output segmentation in same directory where the used network is stored
            sitk.WriteImage(outimt, outnamet.replace('.mhd', '_{}.mhd'.format('9')), True)

os.path.realpath('/content') + os.path.sep + 'Segmentation' + os.path.sep + 'experiments' + os.path.sep + tag

if mode == 'train':
    print("Going to train")
    expdir = os.path.realpath('/content') + os.path.sep + 'Segmentation' + os.path.sep + 'experiments' + os.path.sep + tag
    if not os.path.exists(expdir):
        os.makedirs(expdir)
    train3D(tag, train_dir, fold, lr, lr_step_size, lr_gamma, n_iterations, expdir)
    for item in glob.glob(trained_networks):
          shutil.copy(item, os.path.abspath('./gdrive/MyDrive/490 Heart/modelTr') + os.path.sep + str(n_iterations - 1) + '.pt' )

if mode == 'test':
        print("Going to test")
        netdir, netbase = os.path.split(trained_networks)
        print(trained_networks)
        # TODO: Rework Trained Networks to be a array
        net_ary = []
        net_ary.append(trained_networks)
        print(net_ary)
        test3D(net_ary, test_dir)
        # test3D(trained_networks, test_dir)

        for item in glob.glob('./Segmentation/experiments/testCardiacSegmentationCCTA/*.nii'):
          test_load = nib.load(item).get_fdata()
          print(item,test_load.shape)
          test = test_load[:,:,67]
          plt.imshow(test)
          plt.show()